複数のカメラの認識を使用したインテリジェントなフィッティングルーム
概要
本論文では，店にあるフィッティングルームに対するマルチカメラの認識に基づいて実際の店においてオンラインソーシャルファッション比較を可能にする新しいシステムResponsive Mirrorに対するヴィジョンシステムのアーキテクチャの記述をする．このヴィジョンシステムは自動的に服を試しているようなユーザのモーションを追跡することで”自身”と”ソーシャル”の服の比較のための暗黙的に制御されたリアルタイムインタラクションを提供する．本研究ではモーショントラッキングと服認識の重要な内容を記述し，前のユーザスタディやソーシャルファッションネットワークから集められた画像に対するそれらの有用性を評価する．


   * 背景

      * 実際の店舗における行動

         * 人は購入する服を決定する際に自身の体やイメージにフィットしているかを気にする
         * 重要なのは，ファッションを決定することはそれぞれのソーシャルなコンテキストにおける自己表現としての個人の目的に依存していること
         * その自己表現は多くの複雑で微妙なニュアンスの要因で決定される[3, 4]
      * オンラインショッピングでの行動

         * 服を直接検索したり，推薦されたりする

            * フィルタリグしたり，似たようなコンテンツを表示したり
         * ユーザが”ファッショナブル”だと思うアイテムを提案するのには限界がある

            * ファッションの規則はコンテキストや個人の認識によって変化する”low semanticity"[5]の一つだから
         * そのような推薦は役立つけど，ユーザの”ファッション”に対する認識は扱っていない
      * ファッションにおける技術のほかの役割

         * オンラインファッションソーシャルネットワークの台頭

            * ShareYourLook[20], IQONS[10]
            * カテゴライズしたりタグやコメントをつけられる服装の写真をアップロードすることができる
            * メンバーがタグやコメントをつけることでファッションの意味を構成する
            * システムはただそれができるようにしただけの舞台
      * 本論文

         * 店舗でショッピングする場合の目的を対象とした情報を用いて拡張するシステムを提案

            * 身体にフィットしているか
            * スタイルにフィットしているか
            * この情報を普段のショッピングに対して最小限の混乱だけで提供できるようにしたい
         * 
      * 
   * 提案システム：Responsive Mirror

      * 概念図を図1に示す
      * プロトタイプの写真は図2に示す
      * 様々な服を着たときのソーシャルなコンテキストの反映を示したい
      * 購入者に代わりのスタイルオプションについてインスピレーションのポイントを提供したい

         * 推薦しなくても，店はその購入経験の改善によって長期的な利益が得られるだろう
      * 構成

         * 鏡（真ん中）
         * ディスプレイ×2

            * 左のディスプレイは以前着ていた服装の購入者を表示する鏡

               * 購入者の動作に合わせて服も移動する
               * 服を比較しながら見ることができる
            * 右のディスプレイは類似した服や異なる服を着た他の人の画像が表示される

               * ソーシャルコンテキストの情報や試したそうな代わりの服を購入者に提供する
         * カメラ×2（鏡の上，天井）

            * ユーザ，ディスプレイ間のインタラクションを可能にするリアルタイムビジョンシステムと接続されている
            * ユーザが実際に着替える部屋に設置するのではなく，その隣の部屋に取り付けられる
      * 動作


            * カメラの視界に客がいない場合，ディスプレイにはアンビエントな情報（音楽ビデオ，広告な）が表示されるだけになる
            * 人が入ってくると，その存在を認識し，インタラクティブな状態になる

      1. ヴィジョンシステムは服のスタイルを検出しソーシャルファッションネットワーク内のすべてのコンテンツと比較してマッチングさせる
      2. 左側のディスプレイに小運輸者の前に着ていた服がリアルタイムに移動するユーザに重ねるように表示される

            * 同時に別のディスプレイに表示するために様々なアングルからの客の画像を取り込む
      * 先行研究で全機能を実装した場合を想定したオズによる評価した結果（インタラクションデザインやプロトタイプの一部の機能，システムの潜在的な有用性）を述べている[15]

         * プライバシ，場所，インタラクションの要求
         * 結果

            * 特定の服を購入する可能性に差はなかった
            * 性質上のより良い経験を報告し，購入の決定プロセスにおいてシステムがない場合よりも役立つことがわかっている
      * その他機能


         * Social fashion network

            * 画像類似度サービス（like.com[14]）とスライドショーサービス（RockYou[19]）の組み合わせが最近のトレンド
            * ユーザが提供した写真の中にある服に類似した商品の推薦を示すことができる
            * Responsive Mirrorでは直接推薦する訳ではなく，購入者が試着しているものと類似した，あるいは異なる服を着ているソーシャルネットワーク内の人の写真を提供し，購入者のソーシャルネットワーク内の人が作ったものの表現についての情報を提供する
            * ソーシャルなコンテンツは試着している服を評価している間にフィッティングの様々なコンテキストにおける同様のファッションや代わりのファッションを考慮するのに役立つ
      * マルチカメラビジョンシステムの機構

         * 全体像は図3に示す
         * 鏡の前にいる人の存在を検出し，服を検出・認識，そして体のモーションを追跡する
         * 
   * 関連研究

      * Prada洗練されたドレッシングルーム[4]

         * 各衣服を同定するスキャナを持つ
         * 価格情報や在庫，他の色やサイズの情報を提供する
         * オンラインショップとかで見れるような情報
         * モーショントリガーなカメラも含む
      * Magic Mirror[16]

         * 自身のビデオを友人に送り，コメントや投票（賛成／反対）をもらう
         * 代わりの服を着ているかのような自分の姿（静止画？）がミラー上に表示される
      * どちらも鏡の前での購入者のポーズや動きを検出したり，前のフィッティングからに関連したビューを示したりするのにコンピュータビジョン技術を使用していない
      * さらに，Pradaのシステム[4]はアイテムを同定することができるが，未確認の服を認識することができなかたり，マッチするスタイルを発見することができない
      * 
      * OpenCVのメージャーな内容

      1. 背景のモデリング：カメラの前に人がいないとき，”バックグラウンド モデリング”状態になる
      2. 背景差分を使用した前景マップ検出：初期設定のあと自動的に前景を再検出
      3. モルフォジー画像処理：膨張と収縮．検出に失敗した部分を除去したり，穴を埋めたりする
      4. 購入者の検出：両方のカメラで検出された場合”in view”になる．そのときシステムは服の認識と向きの認識に移行する
      5. 服の検出と認識：購入者が着ている服は購入者のバウンディングボックスを使用して見つける．そのとき，服は様々な特徴（色や袖など）に基づいて様々なカテゴリに分類される．詳細は”Clothes Recgnition"の章で述べる

            * 2つのコンポーネントを結合する必要がある

               * リアルタイムの服の検出・認識エンジンとソーシャルファッションコンテンツ
            * 人を同定する[21]ことや顔を認識[2]を目的としてコンテキストのきっかけとして服の認識を採用する

               * 個人認識したり顔認識したくない
               * [21]，[2]ではグローバルカラーやテクスチャの特徴に基づいてマッチング
               * 上記の2つはファッションのテイストにおける2つの重要な要因である
               * 提案システムでもこの2つの要因を使用する

            * 購入者がシステムとインタラクションしたときのビデオからキャプチャした画像からマッチする画像を探す
            * ユーザスタディではより高精度なマッチングアルゴリズムを使用することを被験者に提案された．

               * たとえば，カラー対クルーネック，存在感とボタンの数（ポロシャツ対フルボタン），パターン（柄）のありなし，パターンの複雑さ（細い格子対太い格子），袖の長さなど
               * 完璧な服認識システムのためにはいくつかの未解決なコンピュータビジョンやマシンラーニング問題をとりあつかわなければならない

                  * たとえば服のスタイルを定義するようなユーザや機械が認識できる顕著な特徴は何か？服を認識するためにこれらの特徴をどのように選択したり組み合わせるか？ユーザのテイストに影響するコンテキストな情報は何か？より良い服の検索のためにこの情報をどのように統合するか？など
                  * 服やファッションの認識に対してこれらの問題がある本研究は役に立たないが，関連したオブジェクトを認識する問題の解決に対して価値がある
            * 本システムでは色とテクスチャとパターン（柄）の分析を行う

            * 服を分類するためにマシンラーニングを使用する
            * 同じカテゴリに所属する服さえマッチングできればよい
            * 図6に本研究で調査している問題の例を示す
            * 服の検出

               * 最初に服（特にスカート）の位置を検出した

                  * スカートの検出することは体の胴体を検出することに相当する
                  * [2]，[21]ではまず人のかをを検出し頭部以下の部分を探す．
                  * 中心のvery narrow clothing boxがよく使用される

                     * この方法は様々なポーズに対して不変量だが，大量の服情報を廃棄したり，購入者の服の着方（ジップ対アンジップ）やアクセサリ（ネックレスやスカーフ）に敏感である
                     * 一般にフィッティングルームでは，鏡の前で人は垂直に立つので，これまでのシステムよりも大きな胴体領域をキャプチャする
                  * 計算量を減らすため，モーショントラッキングによって体の領域を抽出することができるため，だいたいの胴体の割合を体全体のバウンディングボックス内に分割することで効率的に検出することができる
                  * 前景画像（図7a）から体のバウンディングボックスをまず検出する（図7b青）
                  * ざっくり胴体部分がバウンディングボックス内の発見的割合を使用して抽出する（図7b緑）
                  * 不十分な服の位置に対して安定しているので，このシンプルな方法で十分だ
            * 色とテクスチャを用いた服のマッチング

               * 色について

                  * 胴体部分からRGBチャンネルのカラーヒストグラムを計算する
                  * ソーシャルファッションコンテンツのヒストグラムと比較する
                  * 2つの服の類似度はχ2乗検定によって測定される
                  * 同じカテゴリから最も類似した服と異なる服を取り出し，ソーシャル比較ディスプレイに表示する
                  * 抽出結果例を図8に示す
               * テクスチャについて

                  * 顔認識のためによく適用されるアイゲンフェイス法[22]に類似したアイゲンパッチ法を調べる
                  * [21]と同じようにRGBでヒストグラムを構築するかわりに，胴体領域内で重なる小さい画像パッチをクロッピングする
                  * 各パッチは多次元のベクトルで表現される
                  * 全ての服のすべてのパッチがスタックされる
                  * PCA[6]分析を特徴量スタックに適用し，特徴量の次元を減らし，最も重要な重要な特徴量を抽出する
                  * 全ての小さいパッチが第k番目の主要コンポーネントとして射影される（アイゲンパッチ）
                  * 射影された特徴ベクトルのヒストグラムを構築する
                  * 各次元n(n=16)においてヒストグラムのbinはこの次元に従うデータセット内で観測される最大値と最小値の間を均等に分配することで生成される
                  * 初見の服に対しては同様の処理を行いヒストグラムを計算する
                  * 全てのヒストグラムとχ2乗検定を使用して比較する
                  * 抽出結果例を図8bに示す
               * 襟の認識

                  * supervised learningアルゴリズムを使用
                  * 2つの主要な問題に焦点を当てる

                     * 襟の認識と袖の長さの認識
                  * 襟認識にたして特徴量の数を調査したらたいてい顕著なコーナーが襟内の服と比較して多いことがたいていだった
                  * カメラの画像から襟を抽出するために単純な分類器と低レベル特徴点検出器を組み合わせた
                  * ステップ1はHarrisコーナー検出器[9]を使用してRGBチャンネルからコーナー点を検出する

                     * Harris検出器は２番目のモーメントマトリクスを使用する
                     * マトリクスは点xの近傍における勾配分布を記述する
                     * ローカルイメージはxとyの導関数で計算される


                        * IxとIyはスケールがσDのGaussian kernel
                        * 導関数は近傍においてσ1（本実装においてはσD=σ1=6）のガウシアンフィルタで平滑化される
                        * このマトリクスは近傍において2つの主要な信号の変化を示している
                        * この特質によって角の点を抽出することができる

                           * 両方の曲率がとても大きい点で，つまり，信号の変化が直角方向においてsifnificantである
                        * 点xにおけるHarris尺度は式（2）で得られる

                           * Harris尺度は点xにおける曲がり具合の強さつまり，どれくらい特有の過度かを示す指標である
                        * 首部分の各ピクセルにおいてHarris尺度を計算した後，ピークとなる点を半径rの最大でないsuppressionを使用して検出する
                        * ピークとなる点xでHarris尺度が閾値tcよりも高いとき，xはHarris角点としてみんされる
                        * Harris検出器はRGBの各チャンネルに対して適用される
                        * R空間に対してtcを変化させた場合の適用結果例を図9に示す（白い×）
                        * 首部分は体のバウンディングボックスをわけることで発見することができる（図9の緑の四角）
                        * 首部分における全てのチャンネルで検出される仮説のもとHarrisコーナーの数は襟の存在を決定することに使用可能である
                        * 仮にN枚の服{C1, C2, ・・・, CN}が首の部分から検出されたHarrisコーナーの数という特徴で記述されるとすると，

                           * X={x1, x2, ・・・, xN}
                           * そして襟があるなら+1ないなら-1のラベルをつける
                        * 襟は

                           * Y={y1, y2, ．．．, yN} ynは+1か-1
                        * 襟認識はstandard supervised学習問題として定式化される
                        * この問題を解決するために単純のためにsimple Decision Stump分類器をしようした

                           * 1本の枝しかない決定木で構成されるマシンラーニングモデルである
                           * 決定スタンプdは閾値tdと等しいあるいは小さい特徴量を持つ標本を右の枝（-1）に分類する

                              * 他を左の枝（+1）に分類する
                           * 決定スタンプの学習は特徴量をソートし，the drop in impurityを最大化するような特徴量の閾値t*を検索することを含む
      6. 体の向き検出：ユーザの鏡に対する向きの変化を検出したら，フロントカメラがその向きの人の画像を記録し，対応した向きの画像を表示する．体の向きは天井のカメラでモーションをトラッキングすることで検出する．次のセクションで向きの追跡エンジンについて述べる．

            * 上から見ると人は楕円形の形をしている．長い軸の方が肩で，短い軸の方が体の向きとなる．（図4a参照）
            * 白い線は検出した体の外形で最も適した楕円が赤色で表示される
            * 体の向き黄色い線で向きが表現される
            * ただし，人が左下を向いていても黄色の線は180°逆を向くことがある
            * 楕円だけでは後ろと前を区別することができないからである
            * これは顔を検出したり，履歴データから判別することができるが，省略した？（we shall discuss shortly）
            * ポーズによって体の形状に影響がでる問題がある
            * 手を横に広げたり前に出したりすると体の外形が凸状や凹状になる
            * モルフォジー演算の収縮処理によって腕の部分を削る
            * この腕の部分削除は腕の位置を同定できるため，ポーズの評価にも使用することができる
            * 腕を前で組んだ場合，体の外形は円形に近づく
            * 1フレーム前の画像を比較して急激な変化を無視するようにする
            * これは逆向きになることも防ぐ
      7. ポーズマッチング：購入者は様々なポーズで自分を見る傾向にあるため，向きだけでなくポーズも評価する必要がある．詳細は”Pose matching"セッションで述べる

            * フロントカメラと天井カメラの背景差分＋２値化画像を使用する
            * 対応するフロントカメラと天井カメラの特徴において相対的な移動量の最小値を見つけることで並進不変な輝度値の差の2乗和（SSD）を計算
            * 二つの差を足して，一つの差とする
            * 要はフロントカメラの画像と天井カメラの画像の両方で近い画像をとることでポーズのマッチングを行う
            * 他に類似したマッチング画像がなければ前と後ろを間違えてしまう
            * 今後の課題として向きの追跡結果のような特徴量やポーズの類似度の直感によりマッチする差の尺度を差を考慮したい
      * 
   * 実験
   * 結果
   * 考察
   * まとめ




   * 単語

      * inventory：在庫
      * physical store：実際の店舗
      * disruption：混乱
      * crew neck：クルーネック．首にぴったりしたネックラインの基本の形












